
# üìù Publications 
## Video Generation


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/DreamVideo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DreamVideo: Composing Your Dream Videos with Customized Subject and Motion](https://openaccess.thecvf.com/content/CVPR2024/html/Wei_DreamVideo_Composing_Your_Dream_Videos_with_Customized_Subject_and_Motion_CVPR_2024_paper.html) \\
**Yujie Wei**, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, Hongming Shan

[![GitHub Stars](https://img.shields.io/github/stars/damo-vilab/i2vgen-xl?style=social)](https://github.com/damo-vilab/i2vgen-xl)
[![GitHub Forks](https://img.shields.io/github/forks/damo-vilab/i2vgen-xl?style=social)](https://github.com/damo-vilab/i2vgen-xl)
[[Project page]](https://dreamvideo-t2v.github.io/)

- DreamVideo is the first method that generates customized videos from a few static images of the desired subject and a few videos of target motion.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv preprint</div><img src='images/papers/DreamVideo-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control](https://arxiv.org/abs/2410.13830) \\
**Yujie Wei**, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, Hongming Shan

[[Project page]](https://dreamvideo2.github.io/)

- DreamVideo-2 is the first zero-shot (tuning-free) framework that generates customized videos with specified subjects and motion trajectories.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/papers/DreamRelation.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DreamRelation: Relation-Centric Video Customization](https://arxiv.org/abs/2503.07602) \\
**Yujie Wei**,‚ÄâShiwei Zhang, Hangjie Yuan, Biao Gong, Longxiang Tang, Xiang Wang, Haonan Qiu, Hengjia Li, Shuai Tan, Yingya Zhang, Hongming Shan

[[Project page]](https://dreamrelation.github.io/)

- DreamRelation is the first relational video customization method that personalizes user-specified relations.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/papers/TeaCache.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model](https://arxiv.org/abs/2411.19108) \\
Feng Liu, Shiwei Zhang, Xiaofeng Wang, **Yujie Wei**, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, Fang Wan

[[Project page]](https://liewfeng.github.io/TeaCache/)
[[Code]](https://github.com/ali-vilab/TeaCache)

- TeaCache is a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/InstructVideo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[InstructVideo: Instructing Video Diffusion Models with Human Feedback](https://openaccess.thecvf.com/content/CVPR2024/html/Yuan_InstructVideo_Instructing_Video_Diffusion_Models_with_Human_Feedback_CVPR_2024_paper.html) \\
Hangjie Yuan, Shiwei Zhang, Xiang Wang, **Yujie Wei**, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni

[![GitHub Stars](https://img.shields.io/github/stars/damo-vilab/i2vgen-xl?style=social)](https://github.com/damo-vilab/i2vgen-xl)
[![GitHub Forks](https://img.shields.io/github/forks/damo-vilab/i2vgen-xl?style=social)](https://github.com/damo-vilab/i2vgen-xl)
[[Project page]](https://instructvideo.github.io/)

- InstructVideo is the first research attempt that instructs video diffusion models with human feedback.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/HiGen.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Hierarchical Spatio-Temporal Decoupling for Text-to-Video Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Qing_Hierarchical_Spatio-temporal_Decoupling_for_Text-to-Video_Generation_CVPR_2024_paper.html) \\
Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, **Yujie Wei**, Yingya Zhang, Changxin Gao, Nong Sang

[![GitHub Stars](https://img.shields.io/github/stars/damo-vilab/i2vgen-xl?style=social)](https://github.com/damo-vilab/i2vgen-xl)
[![GitHub Forks](https://img.shields.io/github/forks/damo-vilab/i2vgen-xl?style=social)](https://github.com/damo-vilab/i2vgen-xl)
[[Project page]](https://higen-t2v.github.io/)

- HiGen is a method that improves T2V performance by decoupling the spatial and temporal factors from the structure and content level.
</div>
</div>

<div class='paper-box-text' markdown="1">
- `ICCV 2025` [PersonalVideo: High ID-Fidelity Video Customization without Dynamic and Semantic Degradation](https://arxiv.org/abs/2411.17048), Hengjia Li, Haonan Qiu, Shiwei Zhang, Xiang Wang, **Yujie Wei**, Zekun Li, Yingya Zhang, Boxi Wu, Deng Cai.
</div>

## Image Generation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/papers/EvolveDirector.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models](https://arxiv.org/abs/2410.07133) \\
Rui Zhao, Hangjie Yuan, **Yujie Wei**, Shiwei Zhang, Yuchao Gu, Lingmin Ran, Xiang Wang, Zhangjie Wu, Junhao Zhang, Yingya Zhang, Mike Zheng Shou

[[Code]](https://github.com/showlab/EvolveDirector)

- EvolveDirector explores the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/papers/FreeScale.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion](https://arxiv.org/abs/2412.09626) \\
Haonan Qiu, Shiwei Zhang, **Yujie Wei**, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, Ziwei Liu

[[Project page]](http://haonanqiu.com/projects/FreeScale.html)
[[Code]](https://github.com/ali-vilab/FreeScale)

- FreeScale proposes a tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion.

</div>
</div>


## Continual Learning

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/papers/OnPro.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Online Prototype Learning for Online Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Online_Prototype_Learning_for_Online_Continual_Learning_ICCV_2023_paper.html) \\
**Yujie Wei**, Jiaxin Ye, Zhizhong Huang, Junping Zhang, Hongming Shan

[[Code]](https://github.com/weilllllls/OnPro)

- OnPro is the first work to identify shortcut learning as the key limiting factor for online continual learning, offering new insights into why online learning models fail to generalize well.
</div>
</div>

## Speech Emotion Recognition
- `ACM MM 2023` [Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition](https://openreview.net/forum?id=YfwMIDhPccD), Jiaxin Ye, **Yujie Wei**, Xin-Cheng Wen, Chenglong Ma, Zhizhong Huang, Kunhong Liu, Hongming Shan.
- `ICASSP 2023` [Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition](https://ieeexplore.ieee.org/abstract/document/10096370), Jiaxin Ye, Xin-Cheng Wen, **Yujie Wei**, Yong Xu, Kunhong Liu, Hongming Shan.